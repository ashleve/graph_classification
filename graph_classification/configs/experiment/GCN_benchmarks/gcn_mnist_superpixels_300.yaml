# @package _global_

# to execute this experiment run:
# python train.py logger=wandb +experiment/GCN_benchmarks=gcn_mnist_superpixels_300

defaults:
    - override /trainer: default_trainer.yaml
    - override /model: null
    - override /datamodule: null
    - override /optimizer: adam.yaml                    # choose optimizer from 'configs/optimizer/' folder
    - override /seeds: default_seeds.yaml               # choose seeds from 'configs/seeds/' folder
    - override /callbacks: wandb_callbacks.yaml         # choose callback set from 'configs/callbacks/' folder
    - override /logger: null                            # choose logger from 'configs/logger/' folder

seeds:
    pytorch_seed: 12345

trainer:
    args:
        min_epochs: 10
        max_epochs: 50
        accumulate_grad_batches: 1
        gradient_clip_val: 0.5

model:
    class: src.models.graph_classifier.GraphClassifier
    args:
        architecture: GCN
        num_node_features: 3
        num_classes: 10
        conv1_size: 32
        conv2_size: 64
        conv3_size: 512
        pool_method: max
        lin1_size: 64
        dropout1: 0.25

datamodule:
    class: src.datamodules.mnist_superpixels_custom.MnistSuperpixelsCustomDataModule
    args:
        num_nodes: "300"
        batch_size: 32
        train_val_split_ratio: 0.9

optimizer:
    args:
        lr: 0.001
        weight_decay: 0.000001

logger:
    wandb:
        args:
            tags: ["mnist_superpixels_300"]
            notes: ""
            group: ""
