# @package _global_

# to execute this experiment run:
# python train.py logger=wandb +experiment/

defaults:
    - override /trainer: default_trainer.yaml
    - override /model: null
    - override /datamodule: null
    - override /callbacks: default_callbacks.yaml
    - override /logger: csv_logger.yaml

#seed: 1234

trainer:
    min_epochs: 1
    max_epochs: 100
    accumulate_grad_batches: 1
    gradient_clip_val: 0.5

model:
    _target_: src.models.ogbg_ppa_classifier.OGBGPpaClassifier
    architecture: GCN
    optimizer: adam
    lr: 0.001
    weight_decay: 0.0
    node_emb_size: 100
#    edge_emb_size: 100
    node_features: ${model.node_emb_size}
    conv1_size: 256
    conv2_size: 256
    conv3_size: 512
    pool_method: add
    lin1_size: 128
#    dropout1: 0.1
#    dropout2: 0.1
    lin2_size: 32
    output_size: ${datamodule.classes}

datamodule:
    _target_: src.datamodules.ogbg_ppa.OGBGPpa
    data_dir: ${data_dir}
    batch_size: 32
    node_features: "?"
    edge_features: 7
    classes: 37

logger:
    wandb:
        tags: ["ogbg-ppa"]
        notes: ""
        group: ""

callbacks:
    model_checkpoint:
        monitor: "val_acc"
        mode: "max"
    early_stopping:
        monitor: "val_loss"
        mode: "min"
