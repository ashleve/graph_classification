# @package _global_

# example hyperparameter optimization of some experiment with optuna:
# python train.py +experiment=ogbg_molhiv trainer.gpus=1 logger=wandb --config-name config_optuna.yaml --multirun

# specify here default training configuration
defaults:
    # load everything from main config file
    - config.yaml

    # override sweeper to optuna!
    - override hydra/sweeper: optuna


# choose metric optimized by optuna
#optimized_metric: "val_acc_best"
optimized_metric: "val_ap_best"
#optimized_metric: "val_rocauc_best"


hydra:
    # here we define optuna objective
    # it optimizes for value returned from function with @hydra.main decorator
    # learn more here: https://hydra.cc/docs/next/plugins/optuna_sweeper
    sweeper:
        optuna_config:
            study_name: null
            storage: null
            n_jobs: 1
            seed: 123

            # 'minimize' or 'maximize' the objective
            direction: maximize

            # number of experiments that will be executed
            n_trials: 20

            # choose optuna hyperparameter sampler ('tpe', 'random', 'cmaes' or 'nsgaii', 'motpes')
            # learn more here: https://optuna.readthedocs.io/en/stable/reference/samplers.html
            sampler: tpe

        # define range of hyperparameters
        search_space:
            datamodule.batch_size:
                type: categorical
                choices: [128, 64, 32]
            model.activation:
                type: categorical
                choices: ["relu", "prelu"]
            model.conv1_size:
                type: categorical
                choices: [128, 256, 512]
            model.conv2_size:
                type: categorical
                choices: [128, 256, 512]
            model.conv3_size:
                type: categorical
                choices: [null, 128, 256, 512]
            model.conv4_size:
                type: categorical
                choices: [null, 128, 256, 512]
            model.lin1_size:
                type: categorical
                choices: [128, 64]
            model.lin2_size:
                type: categorical
                choices: [64, 32]

#            model.use_batch_norm_after_pooling:
#                type: categorical
#                choices: [False, True]
