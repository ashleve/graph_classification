# @package _global_

# example hyperparameter optimization of some experiment with optuna:
# python train.py +experiment=ogbg_molhiv trainer.gpus=1 logger=wandb --config-name config_optuna.yaml --multirun

# try defaults: -config.yaml

# specify here default training configuration
defaults:
    - trainer: default_trainer.yaml
    - model: graph_classifier.yaml
    - datamodule: mnist_superpixels.yaml
    - callbacks: default_callbacks.yaml  # set this to null if you don't want to use callbacks
    - logger: csv_logger.yaml  # set logger here or use command line (e.g. `python train.py logger=wandb`)

    # add this to enable color logging
    # - hydra/hydra_logging: colorlog
    # - hydra/job_logging: colorlog

    # override sweeper to optuna!
    - override hydra/sweeper: optuna


# path to original working directory (the directory that `train.py` was executed from in command line)
# hydra hijacks working directory by changing it to the current log directory,
# so it's useful to have path to original working directory as a special variable
# read more here: https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
original_work_dir: ${hydra:runtime.cwd}


# path to folder with data
data_dir: ${original_work_dir}/data/


# metric optimized by optuna
optimized_metric: "val_acc_best"
#optimized_metric: "val_ap_best"
#optimized_metric: "val_rocauc_best"


# output paths for hydra logs
hydra:
    run:
        dir: logs/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    sweep:
        dir: logs/multiruns/${now:%Y-%m-%d_%H-%M-%S}
        subdir: ${hydra.job.num}


    # here we define optuna objective
    # it optimizes for value returned from function with @hydra.main decorator
    # learn more here: https://hydra.cc/docs/next/plugins/optuna_sweeper
    sweeper:
        optuna_config:
            study_name: null
            storage: null
            n_jobs: 1
            seed: 123

            # 'minimize' or 'maximize' the objective
            direction: maximize

            # number of experiments that will be executed
#            n_trials: 20
            n_trials: 1

            # choose optuna hyperparameter sampler ('tpe', 'random', 'cmaes' or 'nsgaii', 'motpes')
            # learn more here: https://optuna.readthedocs.io/en/stable/reference/samplers.html
            sampler: tpe

        # define range of hyperparameters
        search_space:
            datamodule.batch_size:
                type: categorical
                choices: [128, 64, 32]
            model.activation:
                type: categorical
                choices: ["relu", "prelu"]
            model.conv1_size:
                type: categorical
                choices: [128, 256, 512]
            model.conv2_size:
                type: categorical
                choices: [128, 256, 512]
            model.conv3_size:
                type: categorical
                choices: [null, 128, 256, 512]
            model.conv4_size:
                type: categorical
                choices: [null, 256, 512]
            model.lin1_size:
                type: categorical
                choices: [128, 64]
            model.lin2_size:
                type: categorical
                choices: [64, 32]
            model.use_batch_norm_after_pooling:
                type: categorical
                choices: [False, True]
