{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch, DataLoader\n",
    "from torchvision.datasets import MNIST, CIFAR10, FashionMNIST\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# add directory above current directory to path\n",
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"FashionMNIST\"\n",
    "ADD_POSITION_TO_FEATURES = True\n",
    "DESIRED_NODES = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load img dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading dataset...\")\n",
    "\n",
    "if DATASET_NAME == \"MNIST\":\n",
    "    trainset = MNIST(\"../data\", download=True, train=True)\n",
    "    testset = MNIST(\"../data\", download=True, train=False)\n",
    "    \n",
    "elif DATASET_NAME == \"CIFAR10\":\n",
    "    trainset = CIFAR10(\"../data/CIFAR10\", download=True, train=True)\n",
    "    testset = CIFAR10(\"../data/CIFAR10\", download=True, train=False)\n",
    "    \n",
    "elif DATASET_NAME == \"FashionMNIST\":\n",
    "    trainset = FashionMNIST(\"../data\", download=True, train=True)\n",
    "    testset = FashionMNIST(\"../data\", download=True, train=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Incorrect dataset name!\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img shape: torch.Size([28, 28])\n",
      "Num of pixels per img: 784\n"
     ]
    }
   ],
   "source": [
    "num_pixels = np.prod(trainset.data.shape[1:])\n",
    "print(\"Img shape:\", trainset.data.shape[1:])\n",
    "print(\"Num of pixels per img:\", num_pixels)\n",
    "assert DESIRED_NODES < num_pixels, ('the number of superpixels cannot exceed the total number of pixels')\n",
    "assert DESIRED_NODES > 1, ('the number of superpixels cannot be too small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n",
      "(70000, 28, 28, 1)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "train_images = trainset.data\n",
    "test_images = testset.data\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "\n",
    "images = np.concatenate((train_images, test_images))\n",
    "if DATASET_NAME == \"MNIST\" or DATASET_NAME == \"FashionMNIST\":\n",
    "    images = np.reshape(images, (len(images), 28, 28, 1))\n",
    "print(images.shape)\n",
    "\n",
    "train_labels = trainset.targets\n",
    "test_labels = testset.targets\n",
    "labels = np.concatenate((train_labels, test_labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show one img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAASGklEQVR4nO3df4yV5ZUH8O8RGX7MAIKD48hMaG34x2wi1RHXVFc2zTYUE4GQmKJp2Kg7jWmTkhCjoX/Uf5roZvuDGNNkWE3p2oVUKYFEXQuk0dQYIhJWUdtVEQQcmBJAht8DnP1jXuyI855z5z73ve+dOd9PQmbmnnnuPfPOPbx37nmf5xFVBRGNfVeVnQAR1QeLnSgIFjtRECx2oiBY7ERBXF3PBxMRvvVfgJaWltzYuXPnzLEDAwO1TudLmpqacmPNzc3m2GPHjtU6nRBUVYa7PanYRWQBgNUAxgH4T1V9MuX+yiQy7PGpSNnty1tvvTU39vHHH5tjDxw4UOt0vuSGG27Ijd12223m2BdeeKHW6YRW9ct4ERkH4BkA3wVwE4BlInJTrRIjotpK+Zt9HoCPVHWPqp4HsB7AotqkRUS1llLsswDsH/L1gey2LxGRbhHZISI7Eh6LiBIV/gadqvYA6AH4Bh1RmVLO7AcBdA75uiO7jYgaUEqxvwVgjoh8XUSaAHwPwObapEVEtSYpbSMRWQjgVxhsvT2nqj9zvr9hX8ZfdZX9/96lS5eqvu+Ojg4z/uCDD5rxlStXmvGpU6eOOKdGcPHiRTN+4cIFM/7YY4+Z8dWrV484p0oV+XxJVUifXVVfBvByyn0QUX3wclmiIFjsREGw2ImCYLETBcFiJwqCxU4URFKffcQPVmKfvci+6M6dO834nDlzzPjEiRPN+OnTp834qVOnqr5vb8748ePHzXh7e7sZnzx5cm7M+7kmTZpkxq15/ABw9OjR3NjWrVvNsQ888IAZ95TZh8/rs/PMThQEi50oCBY7URAsdqIgWOxEQbDYiYIYM603b3XY1J/zzTffzI11dXWZYw8dOmTGJ0yYYMa93MeNG1f1WKs1BvgtJK99Zk1jHT9+vDn2zJkzZtxj3X9ra6s5dtOmTWZ88eLF1aT0Bev5mvpcZeuNKDgWO1EQLHaiIFjsREGw2ImCYLETBcFiJwpizPTZUy1ZssSMb9iwITfm7YTqXQPgTdX0pkNav0NvrBf3cvf68Cn3bV0/APi5W0tRnz171hw7c+ZMM7506VIz/sorr5jxIrHPThQci50oCBY7URAsdqIgWOxEQbDYiYJgsRMFMar67Fbf1dv+1+MdhyNHjuTGrr7a3gzXW465ubnZjHv3b/WbU+f51/P5caUic/e2g/Ye+/rrrzfj3hLb1hoH3u/by72QLZtFZC+AfgAXAVxQVXsVByIqTVKxZ/5ZVfNPe0TUEPg3O1EQqcWuAP4oIm+LSPdw3yAi3SKyQ0R2JD4WESVIfRl/p6oeFJHrAGwRkb+o6utDv0FVewD0AI09EYZorEs6s6vqwexjH4CNAObVIikiqr2qi11EmkVkyuXPAXwHwO5aJUZEtZXyMr4NwMasH3k1gP9W1f+pSVY5Unrp3jrgXi/85MmTubHZs2cn3XfKvGxPynzzsqVeA2A9X7y58tY22IC/pv38+fPN+Pr163NjqdeM5Km62FV1D4Cba5gLERVo9P63T0QjwmInCoLFThQEi50oCBY7URC1mAgzKtxxxx1J45uamnJj3nTIoqffpkxD9XIvU+rPbf1s3u/E20564sSJZtzbxttqvRU1rZhndqIgWOxEQbDYiYJgsRMFwWInCoLFThQEi50oiFG1lHSKffv2mfFp06aZ8RMnTuTGOjs7zbF79uwx41YPH/B7vgMDA7kxb1nilGmigD+F1oqnPrbHOm7eFFXvdzJ16lQzbk2JBvylplNwy2ai4FjsREGw2ImCYLETBcFiJwqCxU4UBIudKIgxM5/95pvthW5bW1vNuNVHB+z5y+fPn696LACcPXvWjHu9bGspam+Zai/u9cJT7z+Fd1ys6w+8efzTp083497vPGX576LwzE4UBIudKAgWO1EQLHaiIFjsREGw2ImCYLETBTFm+uzevG1vi16vn9zc3Jwb8+Zdez1db766N97qN6eMBfw+uTc+Ze12j/fYVq/bez54fXIv946ODjNeBvfMLiLPiUifiOwectsMEdkiIh9mH+0rEIiodJW8jP8NgAVX3PY4gG2qOgfAtuxrImpgbrGr6usAjl5x8yIAa7PP1wJYXNu0iKjWqv2bvU1Ve7PPDwFoy/tGEekG0F3l4xBRjSS/Qaeqai0kqao9AHqAchecJIqu2tbbYRFpB4DsY1/tUiKiIlRb7JsBLM8+Xw5gU23SIaKiuC/jRWQdgPkAWkXkAICfAngSwO9F5CEA+wDcV2SSlbjlllvMuNfL9vrsVk/Xm9vsrVHe0tJixr37t6TOR/d4471+dsrYlPv2evSTJk0y4/39/WbcWzf+9ttvz41t377dHFstt9hVdVlO6Ns1zoWICsTLZYmCYLETBcFiJwqCxU4UBIudKIgxM8W16Kmc1rLEqbzcvemWEyZMyI15UzG9qcGpU1xTeG1D6+cGgM8//zw3Zk1ZBtKnwHq5rVixIje2bFleAywNz+xEQbDYiYJgsRMFwWInCoLFThQEi50oCBY7URBjps/uTTn0eP1ia5qp14NPvQbAY91/kX3wonnH1btGwOrTp/ToAf+4njt3zox723gXYfQ+E4hoRFjsREGw2ImCYLETBcFiJwqCxU4UBIudKIgx02dftWqVGfd6tinzk2fMmGGOPXLkiBn3+vBjlTdn3FtC25trb/3OvKXFves2vKWmveXDFy9enBvzng/ePP88PLMTBcFiJwqCxU4UBIudKAgWO1EQLHaiIFjsREGMmT77jTfeaMa9+cXe/GYrvm/fPnOs15Mtqq862nnHxevDW1thp8yFB/xrBLz737t3b9WPXS33zC4iz4lIn4jsHnLbEyJyUER2Zf8WFpIdEdVMJS/jfwNgwTC3/1JV52b/Xq5tWkRUa26xq+rrAI7WIRciKlDKG3Q/EpF3spf50/O+SUS6RWSHiOxIeCwiSlRtsf8awDcAzAXQC+Dned+oqj2q2qWqXVU+FhHVQFXFrqqHVfWiql4CsAbAvNqmRUS1VlWxi0j7kC+XANid971E1BjcPruIrAMwH0CriBwA8FMA80VkLgAFsBfAD4pL8e9mzZqVG5s8ebI51ptT7o23errevGqvJ5u6B7o13rtvrx+cuu68tT+8t3d86trs06ZNy4156xucPXvWjE+dOtWMe+sjdHZ2mvEiuMWuqsPtDP9sAbkQUYF4uSxRECx2oiBY7ERBsNiJgmCxEwUxqqa43nXXXVWP9do8TU1NZtxqvXltGm+paa8N5E15tNprqdMlG3l6rTfF9fTp07kxryU5ZcoUM+61LL3nhNeOLQLP7ERBsNiJgmCxEwXBYicKgsVOFASLnSgIFjtREKOqz+71oy3edEhvOqW1rPE111yTdN/ez5UyxdUb68W9XnbKFNjUXrOXm9Xr9sZ610Z4uXtTXMvAMztRECx2oiBY7ERBsNiJgmCxEwXBYicKgsVOFMSo6rO/9tprVY9NmRMO2PPhvV6z13P1rgHwerrWz+bNu/bu21sHwNtW2RrvPXZqH976vXjHxYt7v9NGXAeAZ3aiIFjsREGw2ImCYLETBcFiJwqCxU4UBIudKIhR1We/5557qh7rzV/24jNnzsyNHT58OOm+vT691/O1rhHw+sFen9yLe/1kK3fv5/bue/z48Wbcms/u9fBT++ze9QllcM/sItIpIn8SkfdF5D0R+XF2+wwR2SIiH2YfpxefLhFVq5KX8RcArFTVmwD8I4AfishNAB4HsE1V5wDYln1NRA3KLXZV7VXVndnn/QA+ADALwCIAa7NvWwtgcUE5ElENjOhvdhH5GoBvAtgOoE1Ve7PQIQBtOWO6AXQn5EhENVDxu/Ei0gJgA4AVqnpiaEwH30kZ9t0UVe1R1S5V7UrKlIiSVFTsIjIeg4X+O1X9Q3bzYRFpz+LtAPqKSZGIasF9GS+DvZdnAXygqr8YEtoMYDmAJ7OPmwrJcIgFCxZUPdZbrtmbZmpt4fvII4+YY59//nkz7m0X3d/fb8at1pvX9vNaRCnTa724N614woQJZnzixIlmfNq0abkxb7r07Nmzzfjx48fNeIq2tmH/Iv6C1+rNU8nf7N8C8H0A74rIruy2VRgs8t+LyEMA9gG4r6oMiKgu3GJX1T8DyLuy4tu1TYeIisLLZYmCYLETBcFiJwqCxU4UBIudKIhRNcXV6kd7vejm5mYz7vV8LRs3bjTjTz/9tBm///77zbjV4weAa6+9Njf22WefmWO9XrbHO25Wn927BqC1tdWMe9cIbN++PTe2evVqc+zdd99txr2fO+X5dO+995rxNWvWVHW/PLMTBcFiJwqCxU4UBIudKAgWO1EQLHaiIFjsREFIPbeWFZGkB3vxxRdzY0uXLjXH7t+/34x7/ebrrrsuN+Ytt1wmb86318NPXUo6pc9+4sQJM14k7+c6duyYGT9z5owZnz49fzHmrVu3mmO9PryqDvtL45mdKAgWO1EQLHaiIFjsREGw2ImCYLETBcFiJwpiVM1nf/jhh3NjXp998uTJZtzbPrgRt+CthLVtcSXxqD755BMzbm3hDfjrylvXP7zxxhvm2GrxzE4UBIudKAgWO1EQLHaiIFjsREGw2ImCYLETBVHJ/uydAH4LoA2AAuhR1dUi8gSAfwPwt+xbV6nqy0UlCti9S28/ba93ae3lDQDr1q0z42WyrhHwrh/w4qnrHaSMT12b3ZqL7+X16quvmnHrmg/AXyfgpZdeyo099dRT5thqVXJRzQUAK1V1p4hMAfC2iGzJYr9U1f8oJDMiqqlK9mfvBdCbfd4vIh8AmFV0YkRUWyP6m11EvgbgmwAu76vzIxF5R0SeE5Fh19kRkW4R2SEiO9JSJaIUFRe7iLQA2ABghaqeAPBrAN8AMBeDZ/6fDzdOVXtUtUtVu9LTJaJqVVTsIjIeg4X+O1X9AwCo6mFVvaiqlwCsATCvuDSJKJVb7DL4luazAD5Q1V8Mub19yLctAbC79ukRUa1U8m78twB8H8C7IrIru20VgGUiMheD7bi9AH5QQH4V+/TTT824t1S01yrp6OgYcU6XedtFnzp1qur7BuwWVMrWwaPduHHjcmMXLlwwx+7atcuMDwwMmPGWlhYz/swzz5jxIlTybvyfAQzXsCy0p05EtcUr6IiCYLETBcFiJwqCxU4UBIudKAgWO1EQo2opaYu3tfCjjz5qxo8ePWrGe3t7R5zTZefOnat6LFUvZXptX1+fGfe2ZPa2oy7j+gee2YmCYLETBcFiJwqCxU4UBIudKAgWO1EQLHaiICR1qeARPZjI3wDsG3JTK4AjdUtgZBo1t0bNC2Bu1aplbrNVddj9pOta7F95cJEdjbo2XaPm1qh5AcytWvXKjS/jiYJgsRMFUXax95T8+JZGza1R8wKYW7Xqklupf7MTUf2UfWYnojphsRMFUUqxi8gCEfmriHwkIo+XkUMeEdkrIu+KyK6y96fL9tDrE5HdQ26bISJbROTD7OOwe+yVlNsTInIwO3a7RGRhSbl1isifROR9EXlPRH6c3V7qsTPyqstxq/vf7CIyDsD/AfgXAAcAvAVgmaq+X9dEcojIXgBdqlr6BRgi8k8ATgL4rar+Q3bbvwM4qqpPZv9RTlfVxxoktycAnCx7G+9st6L2oduMA1gM4F9R4rEz8roPdThuZZzZ5wH4SFX3qOp5AOsBLCohj4anqq8DuHIJnUUA1mafr8Xgk6XucnJrCKraq6o7s8/7AVzeZrzUY2fkVRdlFPssAPuHfH0AjbXfuwL4o4i8LSLdZSczjDZVvbxG1iEAbWUmMwx3G+96umKb8YY5dtVsf56Kb9B91Z2qeguA7wL4YfZytSHp4N9gjdQ7rWgb73oZZpvxL5R57Krd/jxVGcV+EEDnkK87stsagqoezD72AdiIxtuK+vDlHXSzj/bKiHXUSNt4D7fNOBrg2JW5/XkZxf4WgDki8nURaQLwPQCbS8jjK0SkOXvjBCLSDOA7aLytqDcDWJ59vhzAphJz+ZJG2cY7b5txlHzsSt/+XFXr/g/AQgy+I/8xgJ+UkUNOXjcC+N/s33tl5wZgHQZf1g1g8L2NhwBcC2AbgA8BbAUwo4Fy+y8A7wJ4B4OF1V5Sbndi8CX6OwB2Zf8Wln3sjLzqctx4uSxREHyDjigIFjtRECx2oiBY7ERBsNiJgmCxEwXBYicK4v8BawyFdodcltsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img = images[7]\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test conversion on one img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(196, 3)\n",
      "(1120, 2)\n",
      "(196, 2)\n"
     ]
    }
   ],
   "source": [
    "from utils.img_to_graph import convert_img_to_superpixels_graph\n",
    "\n",
    "\n",
    "x, edge_index, pos = convert_img_to_superpixels_graph(\n",
    "    test_img,\n",
    "    desired_nodes = DESIRED_NODES, \n",
    "    add_position_to_features=ADD_POSITION_TO_FEATURES\n",
    ")\n",
    "print(type(x))\n",
    "print(x.shape)\n",
    "print(edge_index.shape)\n",
    "print(pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert img dataset to superpixel graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images into graphs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7e1d3c6c604e01a11511685362c1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=70000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Took 298.03499603271484s.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from itertools import repeat\n",
    "\n",
    "# apply patch to enable progress bar with multiprocessing, requires python 3.8+\n",
    "# see https://stackoverflow.com/questions/57354700/starmap-combined-with-tqdm/57364423#57364423\n",
    "from utils.img_to_graph import better_istarmap\n",
    "multiprocessing.pool.Pool.istarmap = better_istarmap\n",
    "\n",
    "# method for img -> superpixel_graph conversion\n",
    "from utils.img_to_graph import convert_img_to_superpixels_graph\n",
    "\n",
    "# tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "print(\"Processing images into graphs...\")\n",
    "ptime = time.time()\n",
    "\n",
    "\n",
    "x_list = []\n",
    "edge_index_list = []\n",
    "pos_list = []\n",
    "slices = {\n",
    "    \"x\": [0],\n",
    "    \"edge_index\": [0],\n",
    "    \"pos\": [0],\n",
    "    \"y\": [0],\n",
    "}\n",
    "\n",
    "\n",
    "NUM_WORKERS = 6  # don't use too much or it will crash\n",
    "with multiprocessing.Pool(NUM_WORKERS) as pool:\n",
    "    args = list(zip(images, repeat(DESIRED_NODES), repeat(ADD_POSITION_TO_FEATURES)))\n",
    "    for graph in tqdm(pool.istarmap(convert_img_to_superpixels_graph, args), total=len(args)):\n",
    "        x, edge_index, pos = graph\n",
    "        \n",
    "        x = torch.as_tensor(x, dtype=torch.float32)\n",
    "        edge_index = torch.as_tensor(edge_index, dtype=torch.long)\n",
    "        pos = torch.as_tensor(pos, dtype=torch.float32)\n",
    "        \n",
    "        x_list.append(x)\n",
    "        edge_index_list.append(edge_index)\n",
    "        pos_list.append(pos)\n",
    "        \n",
    "        slices[\"x\"].append(slices[\"x\"][-1] + len(x))\n",
    "        slices[\"edge_index\"].append(slices[\"edge_index\"][-1] + len(edge_index))\n",
    "        slices[\"pos\"].append(slices[\"pos\"][-1] + len(pos))\n",
    "        slices[\"y\"].append(slices[\"y\"][-1] + 1)\n",
    "\n",
    "\n",
    "x_tensor = torch.cat(x_list, dim=0)\n",
    "edge_index_tensor = torch.cat(edge_index_list, dim=0).T\n",
    "pos_tensor = torch.cat(pos_list, dim=0)         \n",
    "y_tensor = torch.as_tensor(labels, dtype=torch.long)\n",
    "\n",
    "slices[\"x\"] = torch.as_tensor(slices[\"x\"], dtype=torch.long)\n",
    "slices[\"edge_index\"] = torch.as_tensor(slices[\"edge_index\"], dtype=torch.long)\n",
    "slices[\"pos\"] = torch.as_tensor(slices[\"pos\"], dtype=torch.long)\n",
    "slices[\"y\"] = torch.as_tensor(slices[\"y\"], dtype=torch.long)\n",
    "\n",
    "del x_list\n",
    "del edge_index_list\n",
    "del pos_list\n",
    "            \n",
    "\n",
    "ptime = time.time() - ptime\n",
    "print(f\"Took {ptime}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13720000, 3])\n",
      "torch.Size([2, 78400000])\n",
      "torch.Size([13720000, 2])\n",
      "torch.Size([70000])\n"
     ]
    }
   ],
   "source": [
    "print(x_tensor.shape)\n",
    "print(edge_index_tensor.shape)\n",
    "print(pos_tensor.shape)\n",
    "print(y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 78400000], pos=[13720000, 2], x=[13720000, 3], y=[70000])\n"
     ]
    }
   ],
   "source": [
    "data = Data(x=x_tensor, edge_index=edge_index_tensor, pos=pos_tensor, y=y_tensor)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset in PyTorch Geometric format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"../data/\" + DATASET_NAME + \"_superpixels_\" + str(DESIRED_NODES) + \"/\" + \"processed\" + \"/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "torch.save((data, slices), path + \"data.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
